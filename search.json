[
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "",
    "text": "The SAS XPORT (XPT) transport file has been the de facto interchange format for SDTM and ADaM datasets in regulatory submissions for decades. While durable and widely supported, XPT is inherently constrained by legacy design decisions and is not a natural “first choice” format within modern open-source data ecosystems. The CDISC Dataset-JSON standard provides a contemporary alternative designed to represent clinical datasets and metadata using JavaScript Object Notation (JSON), enabling straightforward interaction across many programming languages and validation toolchains.\nR Consortium R Submission Pilot 5 explored a fully reproducible, publicly accessible submission-like package centered on R-generated ADaM datasets and delivery of datasets in Dataset-JSON v1.1. The pilot demonstrates (1) converting study data inputs into analysis-ready datasets, (2) serializing datasets to Dataset-JSON using R, (3) regenerating key tables/figures from the R-derived datasets, and (4) implementing quality control comparisons to support confidence that artifacts match expectations and prior baselines.\nThis paper summarizes the background and motivations for Dataset-JSON, the R Consortium Working Group context, and the Pilot 5 implementation and lessons learned for teams evaluating Dataset-JSON as a viable alternative transport format for future submissions."
  },
  {
    "objectID": "paper.html#abstract",
    "href": "paper.html#abstract",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "",
    "text": "The SAS XPORT (XPT) transport file has been the de facto interchange format for SDTM and ADaM datasets in regulatory submissions for decades. While durable and widely supported, XPT is inherently constrained by legacy design decisions and is not a natural “first choice” format within modern open-source data ecosystems. The CDISC Dataset-JSON standard provides a contemporary alternative designed to represent clinical datasets and metadata using JavaScript Object Notation (JSON), enabling straightforward interaction across many programming languages and validation toolchains.\nR Consortium R Submission Pilot 5 explored a fully reproducible, publicly accessible submission-like package centered on R-generated ADaM datasets and delivery of datasets in Dataset-JSON v1.1. The pilot demonstrates (1) converting study data inputs into analysis-ready datasets, (2) serializing datasets to Dataset-JSON using R, (3) regenerating key tables/figures from the R-derived datasets, and (4) implementing quality control comparisons to support confidence that artifacts match expectations and prior baselines.\nThis paper summarizes the background and motivations for Dataset-JSON, the R Consortium Working Group context, and the Pilot 5 implementation and lessons learned for teams evaluating Dataset-JSON as a viable alternative transport format for future submissions."
  },
  {
    "objectID": "paper.html#introduction",
    "href": "paper.html#introduction",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "INTRODUCTION",
    "text": "INTRODUCTION\n\nWhat we are going to talk about\nThis paper covers four related topics:\n\nWhy the industry is reevaluating long-standing dataset transport assumptions (with XPT as the historical default).\nWhat is Dataset-JSON, where it came from, and why it matters.\nThe R Consortium R Submissions Working Group context that enabled Pilot 5.\nR Submission Pilot 5, with emphasis on the Dataset-JSON implementation, reproducible workflows, and what we learned.\n\n\n\nA high-level framing: why XPT feels like yesterday’s solution\nXPT is not “bad”— as it is widely supported and deeply embedded in regulatory processes. However, when viewed through a modern software lens, it unduly burdens the acceptance of modern software practices and adoption of new data standards. In this section, we will lay out why the xpt is problematic.\nOutdated Technology: The format was defined in the 1980s (SAS Technical Report TS-140) and was designed for data transfer between SAS applications on different operating systems, particularly mainframe and VAX systems. It is not aligned with modern data architectures or standards like FHIR or big-data. The FDA adopted it as non-proprietary data format to submit for drug submissions.\nLimited Data Characteristics: Variable Names are restricted to a maximum of 8 characters and are case-insensitive. Variable Labels are limited to 40 characters. Character Variable Lengths cannot exceed 200 characters or bytes. Variable Types are limited to US ASCII for character variables and IBM INTEGER and DOUBLE for numeric variables.\nPoor Handling of International Characters: It only supports US ASCII character encoding, meaning it cannot natively handle multibyte characters or UTF-8. Representing international characters requires “tricks” or workarounds, which can lead to encoding and interpretation issues.\nLack of Robust Metadata: The .xpt format has no direct way to embed rich metadata within the file itself. It relies on separate define.xml files, which introduces the risk of data and metadata mismatches if not carefully managed.\nInefficient Storage: The format is inefficient, often leading to wasted storage space. It allocates fixed space for variables, and missing or shorter values are padded with blanks, which can result in up to 70% wasted space. It also does not support data compression.\nEcosystem asymmetry: XPT is universally known in SAS-centric workflows, but is less native to open-source tooling and data engineering stacks.\nAutomation friction: modern validation, diffing, schema checking, and reproducible pipelines tend to align naturally with text-based and structured formats.\nThis is the motivation for exploring Dataset-JSON: not to declare a villain, but to ask whether an alternative transport can reduce friction and better fit modern, reproducible, open toolchains while continuing to support reviewer needs."
  },
  {
    "objectID": "paper.html#datasetjson",
    "href": "paper.html#datasetjson",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "datasetjson",
    "text": "datasetjson\n\nA brief “history lesson”\nIn 2022, the FDA evaluated alternatives to serve as possible replacements to SAS V5 XPT. The FDA determined that JSON was the optimal modern format to serve as a replacement to SAS V5 XPT. Dataset-JSON is a CDISC standard intended to represent clinical tabular datasets using JSON. It exists in the context of increasing adoption of structured data standards and the need to support interoperability beyond a single vendor ecosystem. It is designed to meet a wide range of data exchange scenarios including regulatory submissions and API-based data sharing.\n\n\nWhat Dataset-JSON is (practically)\nDataset-JSON can be thought of as a standard way to serialize a dataset plus its metadata into JSON. In practical terms it enables:\n\ntransport of rows/records in JSON form\nrepresentation of column metadata (name, label, type, etc.)\nconsistent schema that can be validated and diffed using standard tooling\ncontains one dataset per file\neach dataset can optionally reference a Define-XML file\n\nThe standard addresses requirements from the PHUSE 2017 “Transport for the Next Generation” paper. It follows a lowerCamelCase structure with three main components: top-level metadata (including creation datetime, version, and optional Define-XML references), column metadata (specifying variable names, labels, and data types), and row data arrays. It supports multiple data types with special handling for decimal precision (exchanged as strings to avoid rounding) and ISO 8601 format for date/time variables, with missing values represented as null. The standard also offers an NDJSON format (.ndjson) that enables streaming of large datasets by placing metadata on the first line and individual data rows on subsequent lines, allowing processing without loading entire datasets into memory.\n\n\nRegulatory context and momentum\nIn April 2025, the FDA published a Federal Register notice reinforcing CDISC standards for electronic study data submission, signaling regulatory openness to transport format evolution. This context makes Dataset-JSON increasingly relevant as the industry evaluates alternatives to legacy transport mechanisms while maintaining compliance with CDISC standards.\n\n\nCollaboration framing (CDISC / PHUSE / FDA)\nIndustry adoption of submission changes typically succeeds only where standards bodies, industry groups, and regulators align around feasibility and reviewer usability. Dataset-JSON exists within this collaborative ecosystem where CDISC defines standards, PHUSE supports applied use cases and community practice, and FDA engagement is critical to evaluate submission and review implications.\nWhile this paper is not a policy paper, Pilot 5 is explicitly framed as an FDA–industry collaboration through the R Consortium and serves as a public “show your work” style example."
  },
  {
    "objectID": "paper.html#why-dataset-json-over-other-modern-formats",
    "href": "paper.html#why-dataset-json-over-other-modern-formats",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "WHY DATASET-JSON OVER OTHER MODERN FORMATS",
    "text": "WHY DATASET-JSON OVER OTHER MODERN FORMATS\nWhile modern formats like Parquet offer performance advantages for big data applications, Dataset-JSON provides specific benefits for regulatory submissions:\n\nSelf-contained metadata: Variable labels, types, and CDISC-specific metadata are embedded directly in the file, eliminating external dependencies.\nHuman-readable and audit-friendly: Text-based JSON enables review, comparison, and validation using standard development tools without specialized software.\nVersion control integration: Plain text format works naturally with Git-based workflows and standard diff tools.\nUniversal parsing: JSON parsers exist in virtually every programming language, reducing implementation barriers across toolchains.\nAPI-Based Exchange Dataset-JSON supports both API and file based exchange\n\nBinary formats sacrifice these readability and auditability characteristics—considerations that matter critically in regulatory review contexts where transparency and reproducibility are paramount."
  },
  {
    "objectID": "paper.html#r-consortium-working-group",
    "href": "paper.html#r-consortium-working-group",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "R CONSORTIUM WORKING GROUP",
    "text": "R CONSORTIUM WORKING GROUP\n\nWhat is the focus?\nThe R Consortium R Submissions Working Group has focused on demonstrating that open-source, R-based submission packages can be organized, reproduced, and reviewed in a way that meets regulatory expectations. A key theme across pilots is transparency: making the full submission-like artifact available (data, code, documentation, and outputs), enabling learning and reuse across industry. The mission is:\n\nEasier R-based clinical trial regulatory submissions today\n\nby showing open examples of using current submission portals\n\nEasier R-based clinical trial regulatory submissions tomorrow\n\nby collecting feedback and influencing future industry and agency decisions on system/process setup\n\n\nEach Pilot’s goals as well supporting documentation can be found on the R Consortium’s Submissions Working Group website. The link can be found below.\n\n\nSuccesses (as demonstrated by the pilots)\nAcross the pilot work, the working group has demonstrated:\n\nsubmission-like packaging conventions (eCTD-style organization),\nreproducible execution from raw/source materials through outputs,\npublic documentation (e.g., ADRG) aligning to reviewer needs,\npractical workflows that teams can fork and adapt.\n\nEach Pilot has worked extensively with FDA reviewers to discuss issues from installation of packages, data issues, derivation in R and more. Once the Pilot is completed, the FDA provides recognition of the successful review with a “Statistical Review and Evaluation” letter. Pilot 3’s letter is linked in the Reference materials.\n\n\nProblems identified and iterations\nExploring new transport mechanisms and reproducible workflows surfaces practical issues that do not always appear in “standards only” discussions, including:\n\nensuring variable labels, formats, and metadata persist predictably across conversions,\nmanaging numeric precision and representation choices,\naligning generated deliverables with reviewer expectations and available tooling,\nmaintaining robust QC evidence when changing transport mechanisms.\n\nPilot 5 contains concrete code and QC artifacts illustrating how these issues can be surfaced, addressed, and documented in a public workflow. Pilot 5 uses Dataset-JSON v1.1 and demonstrates converting and producing these artifacts using R in a transparent manner."
  },
  {
    "objectID": "paper.html#pilot-5",
    "href": "paper.html#pilot-5",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "PILOT 5",
    "text": "PILOT 5\n\nFocus of Pilot 5 (Dataset-JSON + R-generated ADaM)\nPilot 5 objectives include delivering a publicly accessible R-based submission package using Dataset-JSON, and expanding prior pilot approaches by generating ADaM datasets using R. For a little bit of history, Pilot 1 created 3 tables and 1 figure all in R, which was submitted to the FDA back in 2021. The SDTM and ADaM datasets used in Pilot 1 were produced in SAS as xpt files. Pilot 3 built on these 3 tables and 1 figure by now rebuilding the ADaM datasets in R and outputting as xpt files. However, there were only 5 datasets needed for the needs of the 3 tables and 1 figure - so only 5 were made in R. Remember the Pilots are more about investigating how the process works and less on the content created in R - hence the small package to help stay focused.\nNow enter Pilot 5. Building off the success of Pilot 1 and Pilot 3 we wanted to see if all the xpt files in the submission package could be converted to datasetjson. The amazing datasetjson R package is available to help do a lot of the heavy lifting. See below for more information on it. As most of the code was in place for the tables, figures and ADaMs, we only had to add minimal code to produce the xpts into datasetjson.\nThe datasetjson R package also had a handy conversion program for moving existing xpts to datasetjson. The Pilots do not have access to the raw data and so the SDTM could not be re-created from scratch. Please see the below links to see Pilot 5 artifacts.\nPrimary public entry points include:\n\nRepo overview: https://github.com/RConsortium/submissions-pilot5-datasetjson\neCTD-like package README (Quarto): https://github.com/RConsortium/submissions-pilot5-datasetjson/blob/main/ectd_readme/README.qmd\n\nADRG previews referenced by the repo README:\n\nHTML: https://rpodcast.quarto.pub/pilot-5-aanalysis-data-reviewers-guide/\nPDF: https://rsubmission-draft.us-east-1.linodeobjects.com/pilot5-adrg-quarto-pdf.pdf\n\n\n\n\nWay of working: reproducibility-first\nPilot 5 is organized to be rerunnable and inspectable, including:\n\ndataset generation programs under pilot5-submission/pilot5-programs/ (e.g., adsl.r, adae.r, adlbc.r, adtte.r),\noutputs and derived artifacts under pilot5-submission/pilot5-output/,\ndocumentation such as ADRG and eCTD README.\n\nMost importantly, Pilot 5 treats comparability as an artifact. For example: - qcReport.qmd compares Pilot 5 ADaM datasets against prior baseline artifacts using diffdf. - tlf-qc.qmd supports comparing generated output artifacts, including text diffs and image-style comparisons.\n\n\nAI infrastructure (where it helped)\nPilot 5 includes experimental automation to assist documentation workflows, including LLM-assisted pipelines for extracting information (e.g., variables, datasets, and filters) from analysis code to support reviewer documentation tables (see adrg/llm-adrg-utils/llm_pipeline.qmd). This is not positioned as a replacement for authoring, but as a mechanism to reduce manual effort and improve consistency when building reviewer-facing documentation from code.\n\n\nSuccess of the Pilot (what Pilot 5 demonstrates)\nPilot 5 demonstrates, with a public reference implementation, that:\n\nDataset-JSON can be produced in an R-driven pipeline with explicit metadata management.\nA submission-like artifact can be packaged in a reviewer-friendly structure.\nOutputs can be regenerated from the provided code and datasets.\nQC comparisons can be scripted and reported to support confidence in the fidelity of the produced artifacts.\n\n\n\nTrials and tribulations (what you run into in the real world)\nPilot 5 encountered a couple of challenges along the way in bringing our submission package to the FDA grouped into standards and technical hurdle.\nFor standards, the first issue was that the standard of datasetjson had just switched from 1.0 to 1.1 so we needed to understand the differences for discussion with FDA. Second, we make use of the Validation software Pinnacle 21 Community v4.1.0 (the latest available at the time of writing), which helps ensure compliance for submissions packages. Unfortunately, the standard 1.1 was not available in either the community or enterprise version of the software. A bit of setback, but we worked with our FDA partners to address and have documented this in the FDA submission. Once the standard 1.1 is available in the Pinnacle 21 we might re-submit.\nFor technical, the first issue was learning how the metadata is applied and stored in the datasetjson format as we have to ensure consistency. The second issue was learning about converting float variables to decimal data type in the JSON output. This was a bit of a thorn in our side in the initial development. Eventually, a wrapper function was built to help provide consistency across all the datasets. There are also a few limitations to the R package datasetjson which are being looked into by the team."
  },
  {
    "objectID": "paper.html#mini-example-from-an-adam-data-frame-to-dataset-json",
    "href": "paper.html#mini-example-from-an-adam-data-frame-to-dataset-json",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "MINI EXAMPLE: FROM AN ADAM DATA FRAME TO DATASET-JSON",
    "text": "MINI EXAMPLE: FROM AN ADAM DATA FRAME TO DATASET-JSON\nThis section provides a minimal illustration of how Dataset-JSON can be produced in R in the same spirit as the Pilot 5 workflow. The goal is to show that Dataset-JSON creation is a reproducible, scriptable step that can be version-controlled and QC’d like any other artifact.\nPilot 5 conversion logic is exemplified by scripts such as pilot5-submission/pilot5-programs/convert_xpt_to_datasetjson.r (read source data, ensure labels, derive column metadata, write Dataset-JSON).\nBelow is a simplified “toy” example mirroring that concept.\n\nlibrary(datasetjson)\nlibrary(tibble)\n\nadsl &lt;- tibble(\n  STUDYID = \"CDISCPILOT01\",\n  USUBJID = c(\"01-701-1015\", \"01-701-1023\"),\n  TRT01A  = c(\"Placebo\", \"Xanomeline Low Dose\"),\n  AGE     = c(67, 72)\n)\n\n# Dataset label (Pilot 5 ensures labels exist; may be derived from source metadata)\nattr(adsl, \"label\") &lt;- \"Subject-Level Analysis Dataset\"\n\n# Minimal column metadata (Pilot 5 derives more comprehensive metadata)\ncolumns &lt;- tibble::tibble(\n  itemOID = names(adsl),\n  name = names(adsl),\n  label = c(\"Study Identifier\", \"Unique Subject Identifier\", \"Actual Treatment for Period 01\", \"Age\"),\n  dataType = c(\"string\", \"string\", \"string\", \"integer\")\n)\n\nds_json &lt;- datasetjson::dataset_json(\n  adsl,\n  item_oid = \"ADSL\",\n  name = \"adsl\",\n  dataset_label = attr(adsl, \"label\"),\n  columns = columns\n)\n\njson_text &lt;- datasetjson::write_dataset_json(ds_json, pretty = TRUE)\n# writeLines(json_text, \"adsl.json\")\ncat(json_text)\n\n{\n  \"datasetJSONCreationDateTime\": \"2026-01-28T18:37:50\",\n  \"datasetJSONVersion\": \"1.1.0\",\n  \"itemGroupOID\": \"ADSL\",\n  \"records\": 2,\n  \"name\": \"adsl\",\n  \"label\": \"Subject-Level Analysis Dataset\",\n  \"columns\": [\n    {\n      \"itemOID\": \"STUDYID\",\n      \"name\": \"STUDYID\",\n      \"label\": \"Study Identifier\",\n      \"dataType\": \"string\"\n    },\n    {\n      \"itemOID\": \"USUBJID\",\n      \"name\": \"USUBJID\",\n      \"label\": \"Unique Subject Identifier\",\n      \"dataType\": \"string\"\n    },\n    {\n      \"itemOID\": \"TRT01A\",\n      \"name\": \"TRT01A\",\n      \"label\": \"Actual Treatment for Period 01\",\n      \"dataType\": \"string\"\n    },\n    {\n      \"itemOID\": \"AGE\",\n      \"name\": \"AGE\",\n      \"label\": \"Age\",\n      \"dataType\": \"integer\"\n    }\n  ],\n  \"rows\": [\n    [\n      \"CDISCPILOT01\",\n      \"01-701-1015\",\n      \"Placebo\",\n      67.0\n    ],\n    [\n      \"CDISCPILOT01\",\n      \"01-701-1023\",\n      \"Xanomeline Low Dose\",\n      72.0\n    ]\n  ]\n}\n\n\nEven in this simplified example, Dataset-JSON highlights two operational advantages that Pilot 5 leverages: - metadata is handled explicitly and can be audited, - the resulting artifact is plain text JSON and fits naturally into diffs, reviews, and automated checks."
  },
  {
    "objectID": "paper.html#conclusion",
    "href": "paper.html#conclusion",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nR Consortium R Submission Pilot 5 provides a concrete, public example demonstrating that Dataset-JSON can be used as a realistic alternative transport format in a submission-like package while maintaining reproducibility and reviewability. The pilot shows that Dataset-JSON can support the same scientific intent and reproducibility expectations traditionally associated with XPT-based submissions. This is achieved by pairing Dataset-JSON with transparent dataset derivation code, explicit metadata management, and reviewer-oriented packaging and documentation (eCTD-like structure + ADRG). First-class QC reporting (dataset and output comparisons) further strengthens confidence in the approach. Together, these elements align more naturally with modern open-source data workflows while maintaining regulatory standards."
  },
  {
    "objectID": "paper.html#references",
    "href": "paper.html#references",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "REFERENCES",
    "text": "REFERENCES\nStatistical Review and Evaluation. (2024). GitHub. https://github.com/RConsortium/submissions-wg/blob/main/_Documents/Summary_R_Pilot3_Submission.pdf\nR Consortium R Submission Pilot 5 development repository. (2025). GitHub.\nhttps://github.com/RConsortium/submissions-pilot5-datasetjson\nR Consortium Pilot 5 eCTD package overview. (2025). GitHub (Quarto source).\nhttps://github.com/RConsortium/submissions-pilot5-datasetjson/blob/main/ectd_readme/README.qmd\nFDA Electronic Common Technical Document (eCTD) overview. (n.d.). U.S. Food and Drug Administration.\nhttps://www.fda.gov/drugs/electronic-regulatory-submission-and-review/electronic-common-technical-document-ectd\nCDISC SDTM & ADaM Pilot Project (CDISCPilot01 source materials). (n.d.). GitHub.\nhttps://github.com/cdisc-org/sdtm-adam-pilot-project\nElectronic Study Data Submission; Data Standards; Clinical Data Interchange Standards Consortium Dataset-JavaScript Object Notation; Request for Comments. Federal Register. (2025, April 9). https://www.federalregister.gov/documents/2025/04/09/2025-06051/electronic-study-data-submission-data-standards-clinical-data-interchange-standards-consortium\nGithub. (2025). Github Copilot (GPT-4.1, 14 April version) [Large Language Model]. https://github.com/copilot/\nOpenAI. (2025). GPT-4.1 (14 April version) [Large language model]. https://platform.openai.com\nOpenAI. (2025). GPT-5.1 (12 November version) [Large language model]. https://platform.openai.com"
  },
  {
    "objectID": "paper.html#recommended-reading",
    "href": "paper.html#recommended-reading",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "RECOMMENDED READING",
    "text": "RECOMMENDED READING\nPilot 5 ADRG preview (public links referenced by the Pilot 5 repo README):\n\nHTML: https://rpodcast.quarto.pub/pilot-5-aanalysis-data-reviewers-guide/\n\nPDF: https://rsubmission-draft.us-east-1.linodeobjects.com/pilot5-adrg-quarto-pdf.pdf\n\nQC reports in the Pilot 5 repo: - qcReport.qmd - tlf-qc.qmd\nLLM-assisted ADRG utilities (Pilot 5 repo): - adrg/llm-adrg-utils/llm_pipeline.qmd\nTS-140 THE RECORD LAYOUT OF A DATA SET IN SAS TRANSPORT (XPORT) FORMAT - https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf"
  },
  {
    "objectID": "paper.html#acknowledgments",
    "href": "paper.html#acknowledgments",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "ACKNOWLEDGMENTS",
    "text": "ACKNOWLEDGMENTS\nThe authors acknowledge the work of the R Consortium and the R Submissions Working Group, as well as the broader set of contributors who made the Pilot 5 repository, documentation, and reproducible workflows publicly available for the benefit of regulators, industry, and the open-source community."
  },
  {
    "objectID": "paper.html#contact-information",
    "href": "paper.html#contact-information",
    "title": "Breaking Free from the Xpt: Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies",
    "section": "CONTACT INFORMATION",
    "text": "CONTACT INFORMATION\nBen Straub\nGSK\nPhiladelphia, United States\nSam Parmar\nPfizer\nNew York City, United States\nNick Masel\nJohnson and Johnson\n(complete location and email as applicable)\nBrand and product names are trademarks of their respective companies."
  },
  {
    "objectID": "paper_feedback.html",
    "href": "paper_feedback.html",
    "title": "phuse-datasetjson",
    "section": "",
    "text": "Certainly! Below is feedback specifically focused on the clarity of your Introduction section (covering from “## INTRODUCTION” to just before “## datasetjson”), referencing the content and structure as presented."
  },
  {
    "objectID": "paper_feedback.html#strengths",
    "href": "paper_feedback.html#strengths",
    "title": "phuse-datasetjson",
    "section": "Strengths",
    "text": "Strengths\n1. Clear Context and Motivation\n- The introduction sets up the problem very clearly. It discusses the long-standing dominance of XPT as a transport format, notes both its durability and its limitations, and highlights how the industry context is changing.\n- The motivation for exploring alternatives is well articulated—the desire for formats that are more compatible with modern, open-source tools and current data standards.\n2. Signposting\n- The explicit list of “What we are going to talk about” provides a helpful roadmap for the paper, letting readers know what to expect in the subsequent sections.\n3. Effective Framing\n- The section “A high-level framing: why XPT feels like yesterday’s solution” sets the scene effectively, summarizing not only what’s old about XPT, but why that matters for today’s workflows.\n- The writing avoids alienating supporters of XPT by clarifying XPT is not “bad,” which positions your argument as thoughtful and measured, rather than adversarial.\n4. Concise, Specific Points\n- The list format (e.g., “Outdated Technology: …”, “Limited Data Characteristics: …”) makes it easy for readers to digest the key limitations of XPT. This breakdown is both organized and accessible. - Each point is supported with concrete details (e.g., variable length restrictions, character encoding issues), which lends credibility and understanding.\n5. Connection to Main Thesis\n- The final sentences in this section (“This is the motivation for exploring Dataset-JSON…”) tie the discussion of XPT’s limitations back to the purpose of your work. This linkage is well-done."
  },
  {
    "objectID": "paper_feedback.html#suggestions-for-improvement",
    "href": "paper_feedback.html#suggestions-for-improvement",
    "title": "phuse-datasetjson",
    "section": "Suggestions for Improvement",
    "text": "Suggestions for Improvement\n1. Redundancy/Transitions\n- There is some repetition between the Abstract and these early Introduction paragraphs. For an audience familiar with the context, you may be able to tighten the focus a bit, reducing overlap.\n- Consider adding very brief transitions between sections—e.g., a linking sentence between the list of what you’ll cover and the start of the XPT critique, to reinforce flow.\n2. Level of Detail\n- Your bullet points (on XPT limitations) are clear, but some could be condensed further, or combined, if brevity is prized.\n- The “Ecosystem asymmetry” and “Automation friction” bullets could be a bit more concrete: consider a short example, or a sentence explaining how text-based formats like JSON facilitate automation.\n3. Balance of Tone\n- The phrase “to ask whether an alternative transport can reduce friction and better fit modern, reproducible, open toolchains while continuing to support reviewer needs” is clear, but would be even stronger if you directly foreshadow that your paper will present concrete evidence or a case study supporting this, to further motivate the reader.\n4. Roadmap Placement\n- The list “This paper covers four related topics:” is located after several paragraphs of introductory text. Some readers may expect a high-level outline a bit earlier; consider moving this up sooner (perhaps just before the “why XPT feels like yesterday’s solution” subsection).\n5. Minor Typos/Clarity - “xpt” should be consistently capitalized as “XPT”. - A few sentences are a bit long or conversational, for instance: “In this section, we will lay out why the xpt is problematic.” For a publication, it can be tightened to: “This section outlines XPT’s limitations.”\n- One typo: “rember” should be “remember”."
  },
  {
    "objectID": "paper_feedback.html#overall-impression",
    "href": "paper_feedback.html#overall-impression",
    "title": "phuse-datasetjson",
    "section": "Overall Impression",
    "text": "Overall Impression\n\nThe introduction is clear, well organized, and sets up the paper’s purpose effectively.\n\nIt contextualizes both the technical and regulatory/compliance issues and succinctly motivates the need for an alternative.\nThe use of signposting and bullet points improves readability.\nJust a little tightening and more active transitions would make it even better.\n\n\nSummary Table\n\n\n\n\n\n\n\n\nAspect\nClarity\nSuggested Action\n\n\n\n\nMotivation\nVery clear\n-\n\n\nProblem definition\nConcrete, well described\n-\n\n\nOrganization\nStrong bullet points\nMove roadmap up earlier\n\n\nTone/Balance\nObjective, professional\nEnsure consistent capitalization/terms\n\n\nTransitions\nAdequate\nAdd linking sentences for flow\n\n\nLevel of detail\nAppropriate\nSlight tightening of overlap\n\n\n\n\nIf you revise with these suggestions in mind (especially with transitions, placement of the roadmap, and tightening up small areas of repetition or wordiness), your introduction will be very strong for a Phuse or similar technical audience. Let me know if you have follow-up questions or want feedback on another specific section!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "phuse-datasetjson",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "slides_long.html",
    "href": "slides_long.html",
    "title": "Breaking Free from the XPT",
    "section": "",
    "text": "title: “Breaking Free from the Xpt” subtitle: “Exploration of dataset.json as an Alternative Transport File to Regulatory Agencies” author: - “Ben Straub (GSK)” - “Sam Parmar (Pfizer)” - “Nick Masel (Johnson & Johnson)” format: revealjs: theme: simple incremental: true slide-number: true transition: fade"
  },
  {
    "objectID": "slides_long.html#dataset-json-as-an-alternative-transport-format",
    "href": "slides_long.html#dataset-json-as-an-alternative-transport-format",
    "title": "Breaking Free from the XPT",
    "section": "Dataset-JSON as an Alternative Transport Format",
    "text": "Dataset-JSON as an Alternative Transport Format\n\nFrom SAS XPT to Dataset-JSON\nR Consortium R Submissions Working Group\nPilot 5: R-generated ADaM + Dataset-JSON\nLessons, challenges, and how you can start\n\n\nFrame the talk as a journey: not “SAS vs R” or “XPT vs JSON” but reducing friction and enabling modern, open, reproducible workflows while still meeting regulatory needs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]